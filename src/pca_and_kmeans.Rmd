---
title: "pca_and_kmeans"
output: html_document
---

```{r setup, include=FALSE}
#setwd("/Users/eklavya/projects/education/formalEducation/DataScience/DataScienceAssignments/HealthCare/Capstone/")
knitr::opts_knit$set(root.dir = normalizePath("/Users/eklavya/projects/education/formalEducation/DataScience/DataScienceAssignments/HealthCare/Capstone/src/"))  # should change the working directory to 'Users/Me/Docs/Proj'
```


```{python}
import os
os.chdir("/Users/eklavya/projects/education/formalEducation/DataScience/DataScienceAssignments/HealthCare/Capstone/src/")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
from math import isnan

from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


import warnings
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')
sns.set_context('paper')

def print_ln():
    print('-' * 80, '\n')


```


# PCA on a group of measures

```{python}
effe_master = pd.read_csv("effe_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
effe_master.info()

print_ln()

eff = effe_master.drop(effe_master.iloc[:, 0:7], axis=1)
eff.columns

```


```{python}
eff = eff.dropna(thresh= 3)

eff.to_csv("eff_data_py.csv")

eff.info()
```


```{python}
eff = eff.apply(lambda x: x.fillna(x.median()), axis=0)
eff

```


```{python}

## With Incremental PCA

from sklearn.decomposition import IncrementalPCA, PCA
pca = IncrementalPCA()
eff_pca = pca.fit_transform(eff)
eff_pca = pd.DataFrame(eff_pca, columns=eff.columns)
eff_pca.index = eff.index
eff_pca


```

```{python}
pca.explained_variance_ratio_
```


```{python}
eff_pca_weights = pd.DataFrame(pca.components_, columns=eff.columns)
eff_pca_weights

```



```{python}

eff_measure_weight = eff_pca.mean(axis=1)
eff_measure_weight


```

```{python}

eff_scores = pd.DataFrame({'effe' : eff_measure_weight})
eff_scores
```
```{python}
def function_group_score(numeric_df, score_name):
    df = numeric_df.dropna(thresh= 3)
    imputed_df = df.apply(lambda x: x.fillna(x.median()), axis=0)
    pca = IncrementalPCA()
    df_pca = pca.fit_transform(imputed_df)
    df_pca = pd.DataFrame(df_pca, columns= df.columns)
    df_pca.index = df.index
    df_with_weight = df_pca.mean(axis=1)
    df_scores = pd.DataFrame({score_name : df_with_weight})
    return df_scores


```

```{python}

effe_master = pd.read_csv("effe_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
eff = effe_master.drop(effe_master.iloc[:, 0:7], axis=1)

effectiveness_scores = function_group_score(eff, 'effe_score')
effectiveness_scores

```


```{python}

df = pd.read_csv("read_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
df.columns
print_ln()

num_df = df.drop(df.iloc[:, 0:7], axis=1)
num_df.columns
print_ln()

readmission_scores = function_group_score(num_df, 'radm_score')
readmission_scores

```

```{python}

df = pd.read_csv("mort_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
df.columns
print_ln()

num_df = df.drop(df.iloc[:, 0:7], axis=1)
num_df.columns
print_ln()

mortality_scores = function_group_score(num_df, 'mort_score')
mortality_scores

```

```{python}

df = pd.read_csv("safe_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
df.columns
print_ln()

num_df = df.drop(df.iloc[:, 0:7], axis=1)
num_df.columns
print_ln()

safety_scores = function_group_score(num_df, 'safety_score')
safety_scores

```

```{python}

df = pd.read_csv("expe_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
df.columns
print_ln()

num_df = df.drop(df.iloc[:, 0:7], axis=1)
num_df.columns
print_ln()

experience_scores = function_group_score(num_df, 'expe_score')
experience_scores

```

```{python}

df = pd.read_csv("medi_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
df.columns
print_ln()

num_df = df.drop(df.iloc[:, 0:7], axis=1)
num_df.columns
print_ln()

medical_scores = function_group_score(num_df, 'medi_score')
medical_scores

```



```{python}

df = pd.read_csv("time_master_data_py.csv").drop(['Unnamed: 0'], axis= 1).set_index('Provider ID')
df.columns
print_ln()

num_df = df.drop(df.iloc[:, 0:7], axis=1)
num_df.columns
print_ln()

timeliness_scores = function_group_score(num_df, 'time_score')
timeliness_scores

```
```{python}
merge1= pd.merge(readmission_scores, mortality_scores, on= 'Provider ID')
merge2= pd.merge(merge1, safety_scores, on= 'Provider ID')
merge3= pd.merge(merge2, experience_scores, on= 'Provider ID')
merge4= pd.merge(merge3, medical_scores, on= 'Provider ID')
merge5= pd.merge(merge4, timeliness_scores, on= 'Provider ID')
merge6= pd.merge(merge5, effectiveness_scores, on= 'Provider ID')
group_scores = merge6
group_scores


```
```{python}

group_scores['final_score'] = group_scores.mean(axis=1)
group_scores
```

```{python}

group_scores['final_score'].describe()
```



# Calculation of final scores is now complete

##########################

##########################

# K-means

```{python}
group_scores.drop(['final_score'], axis=1, inplace= True)
group_scores
```





```{python}
group_scores
```





## PCA On all measures

```{python}

cleaned_master_data = pd.read_csv("cleaned_master_data_py.csv")
cleaned_master_data
```




```{python}

cleaned_master_data_numerical_values = cleaned_master_data.set_index('Provider ID')
cleaned_master_data_numerical_values

```



```{python}

# Putting feature variable to X
X = cleaned_master_data_numerical_values.drop(['Hospital overall rating'],axis=1)
#X = pd.DataFrame(StandardScaler().fit_transform(X))

# Putting response variable to y
y = cleaned_master_data_numerical_values['Hospital overall rating']


## With Incremental PCA

from sklearn.decomposition import IncrementalPCA
X_pca = IncrementalPCA(n_components= 3).fit_transform(X)
X_pca
print_ln()

```


```{python}

X_train, X_test, y_train, y_test = train_test_split(X_pca ,y, \
                                                    train_size=0.7, \
                                                    test_size=0.3, \
                                                    random_state=100)

X_train

print_ln()

y_train.values
```




## K-means

## K = 5


```{python}
kmeans_model = KMeans(n_clusters = 3, max_iter=100,random_state = 100)
kmeans_model.fit(X_train)
  
```


```{python}
#Train the model using the training sets y_pred=clf.predict(X_test)

y_pred= kmeans_model.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

```

```{python}
import itertools
model_possibilities = []
for a_pair in itertools.product([1,2,3, 4, 5],[1, 2, 3, 4,5]):
    model_possibilities.append(a_pair)

model_possibilities   
    
```

```{python}
## ad-hoc auto ML

def pca_kmeans(a_pair):
    num_pca, num_kmeans = a_pair
    X_pca = IncrementalPCA(n_components= num_pca).fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X_pca ,y, \
                                                    train_size=0.7, \
                                                    test_size=0.3, \
                                                    random_state=100)

    kmeans_model = KMeans(n_clusters = num_kmeans, max_iter=100,random_state = 100)
    kmeans_model.fit(X_train)

    y_pred= kmeans_model.predict(X_test)

    print("PCA: ", num_pca)
    print("Kmeans: ", num_kmeans)
    print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
    print_ln()

```



```{python}

for a_pair in model_possibilities:
    pca_kmeans(a_pair)



```


    
```{python}

kmean_df= df_kmean
kmean_df.index = pd.RangeIndex(len(kmean_df.index))
kmean_df = pd.concat([kmean_df, pd.Series(kmeans_model.labels_)], axis=1)
kmean_df.columns = ['PC1', 'PC2','ClusterID']
kmean_df

```



```{python}
kmean_df['ClusterID'].value_counts()

#kmean_df['Hospital overall rating'].value_counts()
 
```


```{python}
plt.figure(figsize=(15,15))
sns.scatterplot(x='PC1',y='PC2',hue='ClusterID',legend='full',data=kmean_df, palette='Set1')
```






```{python}
y_pred

print_ln()

y_test

```




```{python}

def kmeans(k=4):
    kmeans_model = KMeans(n_clusters = k, max_iter=100,random_state = 50)
    kmeans_model.fit(df_kmean)

    kmeans_df=df_kmean
    kmeans_df.index = pd.RangeIndex(len(kmeans_df.index))
    
    kmeans_df2 = pd.concat([kmeans_df, pd.Series(kmeans_model.labels_)], axis=1)
    kmeans_df2.columns = ['PC1', 'PC2','ClusterID']
    kmeans_df2.head()

    kmeans_df2['ClusterID'].value_counts()

    #Train the model using the training sets y_pred=clf.predict(X_test)
    kmeans_model.fit(X_train, y_train)

    y_pred= kmeans_model.predict(X_test)


    #print(kmean_model.predict(X_test))


    print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


    print("=== Confusion Matrix ===")
    print(confusion_matrix(y_test, y_pred))
    print_ln()

    print("=== Classification Report ===")
    print(classification_report(y_test, y_pred))
    print_ln()

    kmeans_model_score = cross_val_score(kmeans_model, X, y, cv=10)


    print("=== All AUC Scores ===")
    print(kmeans_model_score)
    print_ln()

    print("=== Mean AUC Score ===")
    print("Mean AUC Score - K-means: ", kmeans_model_score.mean())
    print_ln()


```

